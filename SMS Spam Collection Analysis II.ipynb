{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09cfe263",
   "metadata": {},
   "source": [
    "# SMS Spam Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4362009",
   "metadata": {},
   "source": [
    "### IMPORT PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a43cc635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/julianetess/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "    #data processing\n",
    "import numpy as np \n",
    "    #linear algebra\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "%matplotlib inline\n",
    "    #data visualization\n",
    "import string \n",
    "    #Python's built-in library of possible punctuations\n",
    "from nltk.corpus import stopwords \n",
    "    #library of possible stopwords\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "    #to lemmatize words\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "    #to convert text documents to a matrix of token counts -- for Bag of Words\n",
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "    #for TF-IDF\n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "    #to train the model using Naive Bayes classifier algorithm\n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "    #evaluate precision of model\n",
    "from sklearn.model_selection import train_test_split\n",
    "    #for training/test set\n",
    "from sklearn.pipeline import Pipeline\n",
    "    #will set up all transformations to do on data for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085a49b9",
   "metadata": {},
   "source": [
    "###### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b2356e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms = pd.read_csv(\"SMSSpamCollection.txt\", sep = \"\\t\", names=[\"label\", \"message\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9572c074",
   "metadata": {},
   "source": [
    "###### Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63c167d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f360dadf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5572\n"
     ]
    }
   ],
   "source": [
    "print(len(sms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93dd0be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                 message\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "538afeb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">message</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>4825</td>\n",
       "      <td>4516</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>747</td>\n",
       "      <td>653</td>\n",
       "      <td>Please call our customer service representativ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      message                                                               \n",
       "        count unique                                                top freq\n",
       "label                                                                       \n",
       "ham      4825   4516                             Sorry, I'll call later   30\n",
       "spam      747    653  Please call our customer service representativ...    4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.groupby('label').describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97d2a4f",
   "metadata": {},
   "source": [
    "   **Side Note:** *Feature engineering* is the process of using domain knowledge to extract features (characteristics, properties, attributes) from raw data. The motivation is to use these extra features to improve the quality of results from a machine learning(*ML*) process, compared with supplying only the raw data to the ML process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1495b1f",
   "metadata": {},
   "source": [
    "###### Create a new column to detect how long each message is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cc1a3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms['length'] = sms['message'].apply(len)\n",
    "sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1393d1a",
   "metadata": {},
   "source": [
    "###### Create a visual for length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43c9b3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Frequency'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAD8CAYAAACGsIhGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAASlklEQVR4nO3df7DldX3f8edLVkFIw8/tluxiFuOOhklq3VwVS9NaMRFBhbZqzNi44+xkO1PaaEgbF5spaTudwZlEhE7LuIGYxRh/oZGtkBhcSTL5Q3BXHUCQsirIriA3ipCoCRLf/eN8LnvYLHzOcu855957no+ZM+f7+Xw/53zf57vf3dd+f5zvSVUhSdJTeca0C5AkLX+GhSSpy7CQJHUZFpKkLsNCktRlWEiSusYWFkl+N8mDSW4f6jspyY1J7m7PJ7b+JLkiyb4ktybZPPSaLW383Um2jKteSdKTG+eexe8B5xzStx3YXVWbgN2tDfBqYFN7bAOuhEG4AJcALwVeAlyyEDCSpMkZW1hU1Z8D3z6k+3xgZ5veCVww1H9NDXwWOCHJqcCrgBur6ttV9RBwI38/gCRJY7ZmwstbV1X3t+kHgHVtej1w39C4/a3vyfqf0imnnFIbN25cdLGSNEv27t37l1W19nDzJh0Wj6uqSrJk9xpJso3BISye85znsGfPnqV6a0maCUnufbJ5k74a6pvt8BLt+cHWfwA4bWjchtb3ZP1/T1XtqKq5qppbu/awwShJepomHRa7gIUrmrYA1w31v6VdFXUm8HA7XPUp4OeTnNhObP9865MkTdDYDkMl+SDwcuCUJPsZXNV0KfCRJFuBe4E3tuE3AOcC+4DvAW8FqKpvJ/kfwOfauP9eVYeeNJckjVlW4y3K5+bmynMWknRkkuytqrnDzfMb3JKkLsNCktRlWEiSugwLSVKXYSFJ6praN7hXso3br39C+55Lz5tSJZI0Ge5ZSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpayphkeRXk3wpye1JPpjkmCSnJ7k5yb4kH07yrDb26Nbe1+ZvnEbNkjTLJh4WSdYDvwLMVdVPAUcBbwLeBVxWVc8DHgK2tpdsBR5q/Ze1cZKkCZrWYag1wLOTrAGOBe4HXgFc2+bvBC5o0+e3Nm3+2UkyuVIlSRMPi6o6APwW8HUGIfEwsBf4TlU91obtB9a36fXAfe21j7XxJx/6vkm2JdmTZM/8/Px4P4QkzZhpHIY6kcHewunAjwHHAecs9n2rakdVzVXV3Nq1axf7dpKkIdM4DPVK4GtVNV9VPwA+DpwFnNAOSwFsAA606QPAaQBt/vHAtyZbsiTNtmmExdeBM5Mc2849nA3cAdwEvL6N2QJc16Z3tTZt/meqqiZYryTNvGmcs7iZwYnqzwO3tRp2AO8ALkqyj8E5iavbS64GTm79FwHbJ12zJM26Nf0hS6+qLgEuOaT7q8BLDjP2b4A3TKIuSdLh+Q1uSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdU3lrrOrzcbt1z+hfc+l502pEkkaD/csJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkrqmERZITklyb5MtJ7kzysiQnJbkxyd3t+cQ2NkmuSLIvya1JNk+jZkmaZdPas7gc+OOqegHwQuBOYDuwu6o2AbtbG+DVwKb22AZcOflyJWm2jRQWSX56qRaY5HjgnwNXA1TVo1X1HeB8YGcbthO4oE2fD1xTA58FTkhy6lLVI0nqG3XP4v8kuSXJv2//2C/G6cA88L4kX0hyVZLjgHVVdX8b8wCwrk2vB+4bev3+1vcESbYl2ZNkz/z8/CJLlCQNGyksqupngTcDpwF7k/xBkp97mstcA2wGrqyqFwHf5eAhp4XlFVBH8qZVtaOq5qpqbu3atU+zNEnS4Yx8zqKq7gZ+A3gH8C+AK9oJ6n99hMvcD+yvqptb+1oG4fHNhcNL7fnBNv8Ag5BasKH1SZImZNRzFv84yWUMTkS/AnhtVf1km77sSBZYVQ8A9yV5fus6G7gD2AVsaX1bgOva9C7gLe2qqDOBh4cOV0mSJmDNiOP+F3AV8M6q+v5CZ1V9I8lvPI3l/kfgA0meBXwVeCuD4PpIkq3AvcAb29gbgHOBfcD32lhJ0gSNGhbnAd+vqr8DSPIM4Jiq+l5Vvf9IF1pVXwTmDjPr7MOMLeDCI12GJGnpjBoWnwZeCfx1ax8L/AnwT8dR1HKzcfv10y5BkqZq1BPcx1TVQlDQpo8dT0mSpOVm1LD47vBtNpL8DPD9pxgvSVpFRj0M9Xbgo0m+AQT4R8AvjKsoSdLyMlJYVNXnkrwAWLjc9a6q+sH4ypIkLSej7lkAvBjY2F6zOQlVdc1YqpIkLSsjhUWS9wM/AXwR+LvWXYBhIUkzYNQ9izngjPadB0nSjBk1LG5ncFLb22ysAId+L+SeS8+bUiWSVotRw+IU4I4ktwB/u9BZVa8bS1WSpGVl1LD4zXEWIUla3ka9dPbPkvw4sKmqPp3kWOCo8ZYmSVouRr1F+S8z+N2J97au9cAnxlSTJGmZGfV2HxcCZwGPwOM/hPQPx1WUJGl5GTUs/raqHl1oJFnDEf7sqSRp5Ro1LP4syTuBZ7ff3v4o8H/HV5YkaTkZNSy2A/PAbcC/Y/DrdU/nF/IkSSvQqFdD/RD4nfaQJM2YUe8N9TUOc46iqp675BVJkpadI7k31IJjgDcAJy19OauDt9uQtNqMdM6iqr419DhQVe8B/BdQkmbEqIehNg81n8FgT+NIfgtDkrSCjfoP/m8PTT8G3AO8ccmrkSQtS6NeDfUvx12IJGn5GvUw1EVPNb+q3r005UiSlqMjuRrqxcCu1n4tcAtw9ziKkiQtL6OGxQZgc1X9FUCS3wSur6p/O67CJEnLx6i3+1gHPDrUfrT1SZJmwKh7FtcAtyT5w9a+ANg5lookScvOqFdD/c8kfwT8bOt6a1V9YXxlSZKWk1EPQwEcCzxSVZcD+5OcPqaaJEnLzKg/q3oJ8A7g4tb1TOD3x1WUJGl5GXXP4l8BrwO+C1BV3wD+wbiKkiQtL6OGxaNVVbTblCc5bnwlSZKWm1HD4iNJ3guckOSXgU/jDyFJ0szoXg2VJMCHgRcAjwDPB/5rVd24mAUnOQrYAxyoqte0E+YfAk4G9gK/VFWPJjmawaW7PwN8C/iFqrpnMcueNH/fQtJK192zaIefbqiqG6vqP1fVf1psUDRvA+4car8LuKyqngc8BGxt/VuBh1r/ZW2cJGmCRj0M9fkkL16qhSbZwODHk65q7QCvAK5tQ3Yy+OIfwPkc/ALgtcDZbbwkaUJGDYuXAp9N8pUktya5Lcmti1jue4BfB37Y2icD36mqx1p7P7C+Ta8H7gNo8x9u458gybYke5LsmZ+fX0RpkqRDPeU5iyTPqaqvA69aqgUmeQ3wYFXtTfLypXrfqtoB7ACYm5urpXpfSVL/BPcnGNxt9t4kH6uqf7MEyzwLeF2Sc4FjgB8FLmdwpdWatvewATjQxh8ATmPwrfE1wPEMTnRLkiakdxhq+NzAc5digVV1cVVtqKqNwJuAz1TVm4GbgNe3YVuA69r0rtamzf9MO+kuSZqQXljUk0yPwzuAi5LsY3BO4urWfzVwcuu/CNg+5jokSYfoHYZ6YZJHGOxhPLtN09pVVT+6mIVX1Z8Cf9qmvwq85DBj/gZ4w2KWI0lanKcMi6o6alKFSJKWryO5RbkkaUYZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLU1buRoMZg4/brn9C+59LzplSJJI3GPQtJUpdhIUnqMiwkSV2GhSSpy7CQJHUZFpKkLi+dXQUOvRRXkpaaexaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6vDfUMuDPrEpa7gyLFcgbB0qaNMNiBTAcJE3bxM9ZJDktyU1J7kjypSRva/0nJbkxyd3t+cTWnyRXJNmX5NYkmyddsyTNummc4H4M+LWqOgM4E7gwyRnAdmB3VW0Cdrc2wKuBTe2xDbhy8iVL0mybeFhU1f1V9fk2/VfAncB64HxgZxu2E7igTZ8PXFMDnwVOSHLqZKuWpNk21Utnk2wEXgTcDKyrqvvbrAeAdW16PXDf0Mv2t75D32tbkj1J9szPz4+vaEmaQVM7wZ3kR4CPAW+vqkeSPD6vqipJHcn7VdUOYAfA3NzcEb12ufGEtqTlZip7FkmeySAoPlBVH2/d31w4vNSeH2z9B4DThl6+ofVJkiZkGldDBbgauLOq3j00axewpU1vAa4b6n9LuyrqTODhocNVkqQJmMZhqLOAXwJuS/LF1vdO4FLgI0m2AvcCb2zzbgDOBfYB3wPeOtFqJUmTD4uq+gsgTzL77MOML+DCsRYlSXpK3khQktRlWEiSugwLSVKXYSFJ6jIsJEld3qJ8BvjjSpIWyz0LSVKXYSFJ6jIsJEldhoUkqcuwkCR1GRaSpC7DQpLUZVhIkroMC0lSl2EhSeoyLCRJXYaFJKnLGwkexqE33pOkWeeehSSpy7CQJHUZFpKkLsNCktRlWEiSugwLSVKXYSFJ6jIsJEldhoUkqcuwkCR1ebuPGXTo7UzuufS8KVUiaaVwz0KS1GVYSJK6DAtJUpdhIUnqWjEnuJOcA1wOHAVcVVWXTrmkVWOU3+/wJLg021ZEWCQ5CvjfwM8B+4HPJdlVVXdMt7LZ1QsYw0VaXVZEWAAvAfZV1VcBknwIOB8wLJap3uW5R/prhL3XG07SeK2UsFgP3DfU3g+8dEq1zKTF/tTsuF9/pOExiT2jlb73ZSBr2EoJi64k24BtrfnXSe56mm91CvCXS1PVirdi10XeteSvX/J1sdgaJ22o3hW7XYzBalsXP/5kM1ZKWBwAThtqb2h9j6uqHcCOxS4oyZ6qmlvs+6wGrouDXBcHuS4OmqV1sVIunf0csCnJ6UmeBbwJ2DXlmiRpZqyIPYuqeizJfwA+xeDS2d+tqi9NuSxJmhkrIiwAquoG4IYJLGrRh7JWEdfFQa6Lg1wXB83MukhVTbsGSdIyt1LOWUiSpsiwaJKck+SuJPuSbJ92PeOW5LQkNyW5I8mXkryt9Z+U5MYkd7fnE1t/klzR1s+tSTZP9xMsvSRHJflCkk+29ulJbm6f+cPt4gqSHN3a+9r8jVMtfIklOSHJtUm+nOTOJC+b1e0iya+2vx+3J/lgkmNmdbswLHjC7UReDZwB/GKSM6Zb1dg9BvxaVZ0BnAlc2D7zdmB3VW0Cdrc2DNbNpvbYBlw5+ZLH7m3AnUPtdwGXVdXzgIeAra1/K/BQ67+sjVtNLgf+uKpeALyQwTqZue0iyXrgV4C5qvopBhfXvIlZ3S6qauYfwMuATw21LwYunnZdE14H1zG499ZdwKmt71Tgrjb9XuAXh8Y/Pm41PBh8d2c38Argk0AYfNlqzaHbCIOr8l7Wpte0cZn2Z1ii9XA88LVDP88sbhccvHPESe3P+ZPAq2Zxu6gq9yyaw91OZP2Uapm4trv8IuBmYF1V3d9mPQCsa9OrfR29B/h14IetfTLwnap6rLWHP+/j66LNf7iNXw1OB+aB97VDclclOY4Z3C6q6gDwW8DXgfsZ/DnvZTa3C8Ni1iX5EeBjwNur6pHheTX4L9Kqv1wuyWuAB6tq77RrWQbWAJuBK6vqRcB3OXjICZip7eJEBjcsPR34MeA44JypFjVFhsVA93Yiq1GSZzIIig9U1cdb9zeTnNrmnwo82PpX8zo6C3hdknuADzE4FHU5cEKShe8iDX/ex9dFm3888K1JFjxG+4H9VXVza1/LIDxmcbt4JfC1qpqvqh8AH2ewrczidmFYNDN3O5EkAa4G7qyqdw/N2gVsadNbGJzLWOh/S7v65Uzg4aHDEitaVV1cVRuqaiODP/vPVNWbgZuA17dhh66LhXX0+jZ+VfxPu6oeAO5L8vzWdTaDnwKYue2CweGnM5Mc2/6+LKyLmdsuAE9wLzyAc4H/B3wF+C/TrmcCn/efMTiUcCvwxfY4l8Ex1t3A3cCngZPa+DC4YuwrwG0MrhCZ+ucYw3p5OfDJNv1c4BZgH/BR4OjWf0xr72vznzvtupd4HfwTYE/bNj4BnDir2wXw34AvA7cD7weOntXtwm9wS5K6PAwlSeoyLCRJXYaFJKnLsJAkdRkWkqQuw0KS1GVYSJK6DAtJUtf/B65KBNvVbtGQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sms['length'].plot(bins=75,kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fc1b135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5572.000000\n",
       "mean       80.490309\n",
       "std        59.944527\n",
       "min         2.000000\n",
       "25%        36.000000\n",
       "50%        62.000000\n",
       "75%       122.000000\n",
       "max       910.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.length.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9b6f1d",
   "metadata": {},
   "source": [
    "   Looking at the description of *length*, the smallest message in the data is only 2 characters long while the largest message is 910 characters(with spaces) long! I'm curious to see if the length of a message is a unique feature between spam and ham."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9ea7c68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ok'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms[sms['length']==2]['message'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0dd61d14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"For me the love should start with attraction.i should feel that I need her every time around me.she should be the first thing which comes in my thoughts.I would start the day and end it with her.she should be there every time I dream.love will be then when my every breath has her name.my life should happen around her.my life will be named to her.I would cry for her.will give all my happiness and take all her sorrows.I will be ready to fight with anyone for her.I will be in love when I will be doing the craziest things for her.love will be when I don't have to proove anyone that my girl is the most beautiful lady on the whole planet.I will always be singing praises for her.love will be when I start up making chicken curry and end up makiing sambar.life will be the most beautiful then.will get every morning and thank god for the day because she is with me.I would like to say a lot..will tell later..\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms[sms['length']==910]['message'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562fc38b",
   "metadata": {},
   "source": [
    "### PREPROCESS:\n",
    "   Our data needs to be preprocessed into some sort of numerical feature vector in order for the classification algorithms to work. I will be working with the Bag of Words approach(each unique word in a text will be represented by one number.)\n",
    "   * **In this section:**\n",
    "  - **Remove punctuations**\n",
    "  - **Remove common words(ie.stopwords)**\n",
    "  - **Split each message into individual words and return a list**\n",
    "  - **Lowercase all words**\n",
    "  - **Combine in a function to use for later* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bda8e0",
   "metadata": {},
   "source": [
    "#### 1. Remove Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a20e53b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample message\n"
     ]
    }
   ],
   "source": [
    "sample_sms = 'Sample message!...'\n",
    "no_punc = [char for char in sample_sms if char not in string.punctuation]\n",
    "no_punc = ''.join(no_punc)\n",
    "print(no_punc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b58295",
   "metadata": {},
   "source": [
    "#### 2. Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c16e47ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491ca389",
   "metadata": {},
   "source": [
    "#### 3. Split words into individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48209d54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sample', 'message']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_punc.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe00ec19",
   "metadata": {},
   "source": [
    "#### 4. Lowercase Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01cf7c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_sms = [word.lower() for word in no_punc.split() if word.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20ccee3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sample', 'message']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_sms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58779f6c",
   "metadata": {},
   "source": [
    "#### 5. Combine all into a function to apply to dataframe later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d909910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean(sample_sms):\n",
    "    no_punc = [char for char in sample_sms if char not in string.punctuation]\n",
    "    no_punc = ''.join(no_punc) \n",
    "    return [word.lower() for word in no_punc.split() if word.lower() not in stopwords.words('english')]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba4442b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms2 = sms['message'].head().apply(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bea6298c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [go, jurong, point, crazy, available, bugis, n...\n",
       "1                       [ok, lar, joking, wif, u, oni]\n",
       "2    [free, entry, 2, wkly, comp, win, fa, cup, fin...\n",
       "3        [u, dun, say, early, hor, u, c, already, say]\n",
       "4    [nah, dont, think, goes, usf, lives, around, t...\n",
       "Name: message, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1492f08",
   "metadata": {},
   "source": [
    "###### *NLTK has lots of built-in tools and great documentation on a lot of these methods of normalization. These tools--such as Porter Stemmer--are not always great for using with text-messages due to the way many people text using abbreviations or shorthand. It's for this reason that I will be moving onto the next portion of the preprocess:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32242607",
   "metadata": {},
   "source": [
    "## Vectorization:\n",
    "   Now we'll convert each message, represented as a list of tokens (lemmas) above, into a vector that machine learning models can understand.\n",
    "   This will take place using the bag-of-words model:\n",
    "   \n",
    "   1. **Term frequency**: Count how many times does a word occur in each message\n",
    "   2. **Inverse document frequency**: Weigh the counts, so that frequent tokens get lower weight\n",
    "   3. **L2 norm**: Normalize the vectors to unit length, to abstract from the original text length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba043c42",
   "metadata": {},
   "source": [
    "#### 1. Term Frequency:\n",
    "  Each vector will have as many dimensions as there are unique words in the sms collection. First, we'll use SciKit Learn's **CountVectorizer**. This model will convert a collection of text documents to a matrix of token counts.\n",
    "   \n",
    "   Since there are so many messages, we can expect a lot of zero counts for the presence of that word in that document. Because of this, SciKit Learn will output a *Sparse Matrix*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f757b1f",
   "metadata": {},
   "source": [
    "### Bag of Words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39cbcf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9530\n"
     ]
    }
   ],
   "source": [
    "bow_transformer = CountVectorizer(analyzer=text_clean).fit(sms['message'])\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c01ad4a",
   "metadata": {},
   "source": [
    "   - Look at one message and get its bag-of-words counts as a vetor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04af331e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U dun say so early hor... U c already then say...\n"
     ]
    }
   ],
   "source": [
    "sms3 = sms['message'][3]\n",
    "print(sms3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d3d300",
   "metadata": {},
   "source": [
    "   - *Vector representation:*\n",
    "   This will show how many unique words are in message number 4 (after removing common stopwords). Some words may appear more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fa28f69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1129)\t1\n",
      "  (0, 1910)\t1\n",
      "  (0, 3002)\t1\n",
      "  (0, 3023)\t1\n",
      "  (0, 4219)\t1\n",
      "  (0, 7208)\t2\n",
      "  (0, 8626)\t2\n",
      "(1, 9530)\n"
     ]
    }
   ],
   "source": [
    "bow3 = bow_transformer.transform([sms3])\n",
    "print(bow3)\n",
    "print(bow3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f267297",
   "metadata": {},
   "source": [
    "    - Looks like there are seven unique words with two words appearing twice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf91aa10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "say\n",
      "u\n"
     ]
    }
   ],
   "source": [
    "print(bow_transformer.get_feature_names_out()[7208])\n",
    "print(bow_transformer.get_feature_names_out()[8626])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ab01a9",
   "metadata": {},
   "source": [
    "   - Use '.transform' on our bag-of-words (*bow*) transformed object and transform the entire dataframe of messages. Check out how the bow counts for the entire sms collection in a large, sparse matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "afa5bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_bow = bow_transformer.transform(sms['message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdda0899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Sparse Matrix: (5572, 9530)\n",
      "Amount of non-zero occurences: 50101\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Sparse Matrix:', sms_bow.shape)\n",
    "print('Amount of non-zero occurences:', sms_bow.nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c152d67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity:0\n"
     ]
    }
   ],
   "source": [
    "sparsity = (100.0 * sms_bow.nnz/(sms_bow.shape[0] * sms_bow.shape[1]))\n",
    "print('sparsity:{}'.format(round(sparsity)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3874071d",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "   \"*Term Frequency-Inverse Document Frequency*\"\n",
    "   \n",
    "   The tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the collection. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.\n",
    "   \n",
    "   Typically, the tf-idf weight is composed by two terms: \n",
    "   1. The first computes the normalized *Term Frequency* (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document. All terms are considered equally important.\n",
    "         \n",
    "        a. TF(t) = (Number of times term t appears in a document)/(Total number of terms in the document)\n",
    "       \n",
    "       \n",
    "   2. The second term is the *Inverse Document Frequency* (IDF), computed as the logarithm of the number of the documents in the collection divided by the number of documents where the specific term appears. This measures how important a term is.\n",
    "\n",
    "        a. IDF(t) = log_e(Total number of documents/Number of documents with term t in it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f056343e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8626)\t0.31949158769666564\n",
      "  (0, 7208)\t0.5597000940482765\n",
      "  (0, 4219)\t0.46452762500729494\n",
      "  (0, 3023)\t0.33557436526898854\n",
      "  (0, 3002)\t0.30912546538910485\n",
      "  (0, 1910)\t0.2870370340589675\n",
      "  (0, 1129)\t0.27985004702413824\n"
     ]
    }
   ],
   "source": [
    "tfidf_transformer = TfidfTransformer().fit(sms_bow)\n",
    "tfidf3 = tfidf_transformer.transform(bow3)\n",
    "print(tfidf3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66a9b03",
   "metadata": {},
   "source": [
    "   - Next, check what is the IDF of the words \"say\" and \"u\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfbcdc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.137052417837396\n",
      "2.932365119299588\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_transformer.idf_[bow_transformer.vocabulary_['say']])\n",
    "print(tfidf_transformer.idf_[bow_transformer.vocabulary_['u']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86ab96cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 9530)\n"
     ]
    }
   ],
   "source": [
    "sms_tfidf = tfidf_transformer.transform(sms_bow)\n",
    "print(sms_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fbd3b3",
   "metadata": {},
   "source": [
    "##### Training the Model: \n",
    "   - With 'message' represented as vectors, we can finally train our spam/ham classifier. I am choosing to use the Naive Bayes classifier algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "adbddedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_detect_model = MultinomialNB().fit(sms_tfidf,sms['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "476c1a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: ham\n",
      "Expected: ham\n"
     ]
    }
   ],
   "source": [
    "print('Predicted:', spam_detect_model.predict(tfidf3)[0])\n",
    "print('Expected:',sms.label[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc5f9748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'ham' 'spam' ... 'ham' 'ham' 'ham']\n"
     ]
    }
   ],
   "source": [
    "all_predictions = spam_detect_model.predict(sms_tfidf)\n",
    "print(all_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a37f038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.97      1.00      0.99      4825\n",
      "        spam       1.00      0.83      0.91       747\n",
      "\n",
      "    accuracy                           0.98      5572\n",
      "   macro avg       0.99      0.92      0.95      5572\n",
      "weighted avg       0.98      0.98      0.98      5572\n",
      "\n",
      "[[4825    0]\n",
      " [ 125  622]]\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(sms['label'],all_predictions))\n",
    "print(confusion_matrix(sms['label'],all_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4c28fa",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e98519",
   "metadata": {},
   "source": [
    "   - Our goal is to accurately predict which of the messages are *spam* and which are *ham*. When completed, we should be able to take completely new data and separate each message as *spam* or *ham*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea704fa",
   "metadata": {},
   "source": [
    "##### Data Wrangling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72955f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sms['message']\n",
    "y = sms['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ef75f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x,y,test_size = .2, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65db844",
   "metadata": {},
   "source": [
    "   - In this case, the value of test_size= is .2, because we are going to use a 80/20 train/test split. This means that we are reserving 20% of the data for testing, and training with the remaining 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "45368a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4457,) (4457,)\n",
      "(1115,) (1115,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd60fb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=text_clean)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('classifier', MultinomialNB()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a77d04e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;bow&#x27;,\n",
       "                 CountVectorizer(analyzer=&lt;function text_clean at 0x16aa4e4c0&gt;)),\n",
       "                (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
       "                (&#x27;classifier&#x27;, MultinomialNB())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;bow&#x27;,\n",
       "                 CountVectorizer(analyzer=&lt;function text_clean at 0x16aa4e4c0&gt;)),\n",
       "                (&#x27;tfidf&#x27;, TfidfTransformer()),\n",
       "                (&#x27;classifier&#x27;, MultinomialNB())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(analyzer=&lt;function text_clean at 0x16aa4e4c0&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfTransformer</label><div class=\"sk-toggleable__content\"><pre>TfidfTransformer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('bow',\n",
       "                 CountVectorizer(analyzer=<function text_clean at 0x16aa4e4c0>)),\n",
       "                ('tfidf', TfidfTransformer()),\n",
       "                ('classifier', MultinomialNB())])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f17a50b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipeline.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e724c46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       1.00      0.95      0.98      1027\n",
      "        spam       0.65      1.00      0.79        88\n",
      "\n",
      "    accuracy                           0.96      1115\n",
      "   macro avg       0.83      0.98      0.88      1115\n",
      "weighted avg       0.97      0.96      0.96      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(predictions,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0e6d5c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.957847533632287\n"
     ]
    }
   ],
   "source": [
    "print(\"Score:\", pipeline.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83de9cbe",
   "metadata": {},
   "source": [
    "   - This means the model is accurate approximately 95% of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e26362c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
